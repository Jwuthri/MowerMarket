{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../../../Python/Mozinor/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technique de construction de modèle utilisé ici est 'Score comparison' (moins propre que le backward ou forward elimination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/az02210/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Reading the file train.csv\n",
      "Read csv file: train.csv\n",
      "args: {'encoding': 'utf-8-sig', 'sep': ',', 'decimal': ',', 'engine': 'python', 'filepath_or_buffer': 'train.csv', 'thousands': '.', 'parse_dates': ['capacity', 'failure_rate', 'id', 'margin', 'price', 'prod_cost', 'warranty', 'attractiveness', 'prod_type_electrique', 'prod_type_essence', 'quality_Low', 'quality_Medium'], 'infer_datetime_format': True}\n",
      "Inital dtypes is capacity                float64\n",
      "failure_rate            float64\n",
      "id                        int64\n",
      "margin                  float64\n",
      "price                   float64\n",
      "prod_cost               float64\n",
      "warranty                  int64\n",
      "attractiveness          float64\n",
      "prod_type_electrique      int64\n",
      "prod_type_essence         int64\n",
      "quality_Low               int64\n",
      "quality_Medium            int64\n",
      "dtype: object\n",
      "Work on PolynomialFeatures: degree 1\n",
      "Optimal number of clusters\n",
      "Optimal number of trees\n",
      "Estimator LassoLarsCV\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    0.1s finished\n",
      "   Best params => {'normalize': False}\n",
      "   Best Score => 0.215\n",
      "Estimator XGBRegressor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1330, 12)\n",
      "\n",
      "    Polynomial Features: generate a new feature matrix\n",
      "    consisting of all polynomial combinations of the features.\n",
      "    For 2 features [a, b]:\n",
      "        the degree 1 polynomial give [a, b]\n",
      "        the degree 2 polynomial give [1, a, b, a^2, ab, b^2]\n",
      "    ...\n",
      "\n",
      "\n",
      "    ELBOW: explain the variance as a function of clusters.\n",
      "\n",
      "\n",
      "    OOB: this is the average error for each training observations,\n",
      "    calculted using the trees that doesn't contains this observation\n",
      "    during the creation of the tree.\n",
      "\n",
      "\n",
      "    LassoLarsCV: performs L1 regularization, it adds a factor of sum of\n",
      "    absolute value of coefficients in the optimization objective.\n",
      "    Usefull with lot of features, made some feature selection.\n",
      "\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "[CV] normalize=True ..................................................\n",
      "[CV] ................................... normalize=True, total=   0.1s\n",
      "[CV] normalize=True ..................................................\n",
      "[CV] ................................... normalize=True, total=   0.0s\n",
      "[CV] normalize=True ..................................................\n",
      "[CV] ................................... normalize=True, total=   0.0s\n",
      "[CV] normalize=False .................................................\n",
      "[CV] .................................. normalize=False, total=   0.0s\n",
      "[CV] normalize=False .................................................\n",
      "[CV] .................................. normalize=False, total=   0.0s\n",
      "[CV] normalize=False .................................................\n",
      "[CV] .................................. normalize=False, total=   0.0s\n",
      "\n",
      "    Gradient boosting is an approach where new models are created that predict\n",
      "    the residuals or errors of prior models and then added together to make\n",
      "    the final prediction. It is called gradient boosting because it uses a\n",
      "    gradient descent algorithm to minimize the loss when adding new models.\n",
      "\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] subsample=0.2, n_estimators=100, min_child_weight=3, max_depth=4, learning_rate=1.0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.2, n_estimators=100, min_child_weight=3, max_depth=4, learning_rate=1.0, total=   0.0s\n",
      "[CV] subsample=0.2, n_estimators=100, min_child_weight=3, max_depth=4, learning_rate=1.0 \n",
      "[CV]  subsample=0.2, n_estimators=100, min_child_weight=3, max_depth=4, learning_rate=1.0, total=   0.0s\n",
      "[CV] subsample=0.2, n_estimators=100, min_child_weight=3, max_depth=4, learning_rate=1.0 \n",
      "[CV]  subsample=0.2, n_estimators=100, min_child_weight=3, max_depth=4, learning_rate=1.0, total=   0.0s\n",
      "[CV] subsample=0.1, n_estimators=100, min_child_weight=6, max_depth=9, learning_rate=0.5 \n",
      "[CV]  subsample=0.1, n_estimators=100, min_child_weight=6, max_depth=9, learning_rate=0.5, total=   0.0s\n",
      "[CV] subsample=0.1, n_estimators=100, min_child_weight=6, max_depth=9, learning_rate=0.5 \n",
      "[CV]  subsample=0.1, n_estimators=100, min_child_weight=6, max_depth=9, learning_rate=0.5, total=   0.0s\n",
      "[CV] subsample=0.1, n_estimators=100, min_child_weight=6, max_depth=9, learning_rate=0.5 \n",
      "[CV]  subsample=0.1, n_estimators=100, min_child_weight=6, max_depth=9, learning_rate=0.5, total=   0.0s\n",
      "[CV] subsample=0.5, n_estimators=10, min_child_weight=9, max_depth=2, learning_rate=1.0 \n",
      "[CV]  subsample=0.5, n_estimators=10, min_child_weight=9, max_depth=2, learning_rate=1.0, total=   0.0s\n",
      "[CV] subsample=0.5, n_estimators=10, min_child_weight=9, max_depth=2, learning_rate=1.0 \n",
      "[CV]  subsample=0.5, n_estimators=10, min_child_weight=9, max_depth=2, learning_rate=1.0, total=   0.0s\n",
      "[CV] subsample=0.5, n_estimators=10, min_child_weight=9, max_depth=2, learning_rate=1.0 \n",
      "[CV]  subsample=0.5, n_estimators=10, min_child_weight=9, max_depth=2, learning_rate=1.0, total=   0.0s\n",
      "[CV] subsample=0.9, n_estimators=50, min_child_weight=6, max_depth=5, learning_rate=1.0 \n",
      "[CV]  subsample=0.9, n_estimators=50, min_child_weight=6, max_depth=5, learning_rate=1.0, total=   0.0s\n",
      "[CV] subsample=0.9, n_estimators=50, min_child_weight=6, max_depth=5, learning_rate=1.0 \n",
      "[CV]  subsample=0.9, n_estimators=50, min_child_weight=6, max_depth=5, learning_rate=1.0, total=   0.0s\n",
      "[CV] subsample=0.9, n_estimators=50, min_child_weight=6, max_depth=5, learning_rate=1.0 \n",
      "[CV]  subsample=0.9, n_estimators=50, min_child_weight=6, max_depth=5, learning_rate=1.0, total=   0.0s\n",
      "[CV] subsample=0.7, n_estimators=10, min_child_weight=9, max_depth=4, learning_rate=0.1 \n",
      "[CV]  subsample=0.7, n_estimators=10, min_child_weight=9, max_depth=4, learning_rate=0.1, total=   0.0s\n",
      "[CV] subsample=0.7, n_estimators=10, min_child_weight=9, max_depth=4, learning_rate=0.1 \n",
      "[CV]  subsample=0.7, n_estimators=10, min_child_weight=9, max_depth=4, learning_rate=0.1, total=   0.0s\n",
      "[CV] subsample=0.7, n_estimators=10, min_child_weight=9, max_depth=4, learning_rate=0.1 \n",
      "[CV]  subsample=0.7, n_estimators=10, min_child_weight=9, max_depth=4, learning_rate=0.1, total=   0.0s\n",
      "[CV] subsample=0.9, n_estimators=10, min_child_weight=5, max_depth=5, learning_rate=0.1 \n",
      "[CV]  subsample=0.9, n_estimators=10, min_child_weight=5, max_depth=5, learning_rate=0.1, total=   0.0s\n",
      "[CV] subsample=0.9, n_estimators=10, min_child_weight=5, max_depth=5, learning_rate=0.1 \n",
      "[CV]  subsample=0.9, n_estimators=10, min_child_weight=5, max_depth=5, learning_rate=0.1, total=   0.0s\n",
      "[CV] subsample=0.9, n_estimators=10, min_child_weight=5, max_depth=5, learning_rate=0.1 \n",
      "[CV]  subsample=0.9, n_estimators=10, min_child_weight=5, max_depth=5, learning_rate=0.1, total=   0.0s\n",
      "[CV] subsample=0.5, n_estimators=50, min_child_weight=3, max_depth=4, learning_rate=0.1 \n",
      "[CV]  subsample=0.5, n_estimators=50, min_child_weight=3, max_depth=4, learning_rate=0.1, total=   0.0s\n",
      "[CV] subsample=0.5, n_estimators=50, min_child_weight=3, max_depth=4, learning_rate=0.1 \n",
      "[CV]  subsample=0.5, n_estimators=50, min_child_weight=3, max_depth=4, learning_rate=0.1, total=   0.0s\n",
      "[CV] subsample=0.5, n_estimators=50, min_child_weight=3, max_depth=4, learning_rate=0.1 \n",
      "[CV]  subsample=0.5, n_estimators=50, min_child_weight=3, max_depth=4, learning_rate=0.1, total=   0.0s\n",
      "[CV] subsample=0.2, n_estimators=75, min_child_weight=10, max_depth=6, learning_rate=0.5 \n",
      "[CV]  subsample=0.2, n_estimators=75, min_child_weight=10, max_depth=6, learning_rate=0.5, total=   0.0s\n",
      "[CV] subsample=0.2, n_estimators=75, min_child_weight=10, max_depth=6, learning_rate=0.5 \n",
      "[CV]  subsample=0.2, n_estimators=75, min_child_weight=10, max_depth=6, learning_rate=0.5, total=   0.0s\n",
      "[CV] subsample=0.2, n_estimators=75, min_child_weight=10, max_depth=6, learning_rate=0.5 \n",
      "[CV]  subsample=0.2, n_estimators=75, min_child_weight=10, max_depth=6, learning_rate=0.5, total=   0.0s\n",
      "[CV] subsample=0.6, n_estimators=100, min_child_weight=6, max_depth=1, learning_rate=0.01 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:    0.8s finished\n",
      "   Best params => {'subsample': 0.5, 'n_estimators': 50, 'min_child_weight': 3, 'max_depth': 4, 'learning_rate': 0.1}\n",
      "   Best Score => 0.664\n",
      "Work on PolynomialFeatures: degree 2\n",
      "Optimal number of clusters\n",
      "Optimal number of trees\n",
      "Estimator LassoLarsCV\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.6, n_estimators=100, min_child_weight=6, max_depth=1, learning_rate=0.01, total=   0.0s\n",
      "[CV] subsample=0.6, n_estimators=100, min_child_weight=6, max_depth=1, learning_rate=0.01 \n",
      "[CV]  subsample=0.6, n_estimators=100, min_child_weight=6, max_depth=1, learning_rate=0.01, total=   0.0s\n",
      "[CV] subsample=0.6, n_estimators=100, min_child_weight=6, max_depth=1, learning_rate=0.01 \n",
      "[CV]  subsample=0.6, n_estimators=100, min_child_weight=6, max_depth=1, learning_rate=0.01, total=   0.0s\n",
      "[CV] subsample=0.2, n_estimators=50, min_child_weight=8, max_depth=9, learning_rate=0.01 \n",
      "[CV]  subsample=0.2, n_estimators=50, min_child_weight=8, max_depth=9, learning_rate=0.01, total=   0.0s\n",
      "[CV] subsample=0.2, n_estimators=50, min_child_weight=8, max_depth=9, learning_rate=0.01 \n",
      "[CV]  subsample=0.2, n_estimators=50, min_child_weight=8, max_depth=9, learning_rate=0.01, total=   0.0s\n",
      "[CV] subsample=0.2, n_estimators=50, min_child_weight=8, max_depth=9, learning_rate=0.01 \n",
      "[CV]  subsample=0.2, n_estimators=50, min_child_weight=8, max_depth=9, learning_rate=0.01, total=   0.0s\n",
      "\n",
      "    Polynomial Features: generate a new feature matrix\n",
      "    consisting of all polynomial combinations of the features.\n",
      "    For 2 features [a, b]:\n",
      "        the degree 1 polynomial give [a, b]\n",
      "        the degree 2 polynomial give [1, a, b, a^2, ab, b^2]\n",
      "    ...\n",
      "\n",
      "\n",
      "    ELBOW: explain the variance as a function of clusters.\n",
      "\n",
      "\n",
      "    OOB: this is the average error for each training observations,\n",
      "    calculted using the trees that doesn't contains this observation\n",
      "    during the creation of the tree.\n",
      "\n",
      "\n",
      "    LassoLarsCV: performs L1 regularization, it adds a factor of sum of\n",
      "    absolute value of coefficients in the optimization objective.\n",
      "    Usefull with lot of features, made some feature selection.\n",
      "\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "[CV] normalize=True ..................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................... normalize=True, total=   0.1s\n",
      "[CV] normalize=True ..................................................\n",
      "[CV] ................................... normalize=True, total=   0.1s\n",
      "[CV] normalize=True ..................................................\n",
      "[CV] ................................... normalize=True, total=   0.1s\n",
      "[CV] normalize=False .................................................\n",
      "[CV] .................................. normalize=False, total=   0.1s\n",
      "[CV] normalize=False .................................................\n",
      "[CV] .................................. normalize=False, total=   0.1s\n",
      "[CV] normalize=False .................................................\n",
      "[CV] .................................. normalize=False, total=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    0.5s finished\n",
      "   Best params => {'normalize': False}\n",
      "   Best Score => 0.684\n",
      "Estimator XGBRegressor\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Gradient boosting is an approach where new models are created that predict\n",
      "    the residuals or errors of prior models and then added together to make\n",
      "    the final prediction. It is called gradient boosting because it uses a\n",
      "    gradient descent algorithm to minimize the loss when adding new models.\n",
      "\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] subsample=0.2, n_estimators=50, min_child_weight=7, max_depth=5, learning_rate=0.01 \n",
      "[CV]  subsample=0.2, n_estimators=50, min_child_weight=7, max_depth=5, learning_rate=0.01, total=   0.1s\n",
      "[CV] subsample=0.2, n_estimators=50, min_child_weight=7, max_depth=5, learning_rate=0.01 \n",
      "[CV]  subsample=0.2, n_estimators=50, min_child_weight=7, max_depth=5, learning_rate=0.01, total=   0.1s\n",
      "[CV] subsample=0.2, n_estimators=50, min_child_weight=7, max_depth=5, learning_rate=0.01 \n",
      "[CV]  subsample=0.2, n_estimators=50, min_child_weight=7, max_depth=5, learning_rate=0.01, total=   0.1s\n",
      "[CV] subsample=0.8, n_estimators=10, min_child_weight=8, max_depth=10, learning_rate=0.5 \n",
      "[CV]  subsample=0.8, n_estimators=10, min_child_weight=8, max_depth=10, learning_rate=0.5, total=   0.1s\n",
      "[CV] subsample=0.8, n_estimators=10, min_child_weight=8, max_depth=10, learning_rate=0.5 \n",
      "[CV]  subsample=0.8, n_estimators=10, min_child_weight=8, max_depth=10, learning_rate=0.5, total=   0.1s\n",
      "[CV] subsample=0.8, n_estimators=10, min_child_weight=8, max_depth=10, learning_rate=0.5 \n",
      "[CV]  subsample=0.8, n_estimators=10, min_child_weight=8, max_depth=10, learning_rate=0.5, total=   0.1s\n",
      "[CV] subsample=0.7, n_estimators=100, min_child_weight=8, max_depth=8, learning_rate=0.01 \n",
      "[CV]  subsample=0.7, n_estimators=100, min_child_weight=8, max_depth=8, learning_rate=0.01, total=   0.5s\n",
      "[CV] subsample=0.7, n_estimators=100, min_child_weight=8, max_depth=8, learning_rate=0.01 \n",
      "[CV]  subsample=0.7, n_estimators=100, min_child_weight=8, max_depth=8, learning_rate=0.01, total=   0.5s\n",
      "[CV] subsample=0.7, n_estimators=100, min_child_weight=8, max_depth=8, learning_rate=0.01 \n",
      "[CV]  subsample=0.7, n_estimators=100, min_child_weight=8, max_depth=8, learning_rate=0.01, total=   0.5s\n",
      "[CV] subsample=0.9, n_estimators=50, min_child_weight=7, max_depth=7, learning_rate=0.01 \n",
      "[CV]  subsample=0.9, n_estimators=50, min_child_weight=7, max_depth=7, learning_rate=0.01, total=   0.3s\n",
      "[CV] subsample=0.9, n_estimators=50, min_child_weight=7, max_depth=7, learning_rate=0.01 \n",
      "[CV]  subsample=0.9, n_estimators=50, min_child_weight=7, max_depth=7, learning_rate=0.01, total=   0.3s\n",
      "[CV] subsample=0.9, n_estimators=50, min_child_weight=7, max_depth=7, learning_rate=0.01 \n",
      "[CV]  subsample=0.9, n_estimators=50, min_child_weight=7, max_depth=7, learning_rate=0.01, total=   0.3s\n",
      "[CV] subsample=0.8, n_estimators=10, min_child_weight=4, max_depth=10, learning_rate=0.5 \n",
      "[CV]  subsample=0.8, n_estimators=10, min_child_weight=4, max_depth=10, learning_rate=0.5, total=   0.1s\n",
      "[CV] subsample=0.8, n_estimators=10, min_child_weight=4, max_depth=10, learning_rate=0.5 \n",
      "[CV]  subsample=0.8, n_estimators=10, min_child_weight=4, max_depth=10, learning_rate=0.5, total=   0.1s\n",
      "[CV] subsample=0.8, n_estimators=10, min_child_weight=4, max_depth=10, learning_rate=0.5 \n",
      "[CV]  subsample=0.8, n_estimators=10, min_child_weight=4, max_depth=10, learning_rate=0.5, total=   0.1s\n",
      "[CV] subsample=0.5, n_estimators=50, min_child_weight=7, max_depth=7, learning_rate=0.01 \n",
      "[CV]  subsample=0.5, n_estimators=50, min_child_weight=7, max_depth=7, learning_rate=0.01, total=   0.2s\n",
      "[CV] subsample=0.5, n_estimators=50, min_child_weight=7, max_depth=7, learning_rate=0.01 \n",
      "[CV]  subsample=0.5, n_estimators=50, min_child_weight=7, max_depth=7, learning_rate=0.01, total=   0.2s\n",
      "[CV] subsample=0.5, n_estimators=50, min_child_weight=7, max_depth=7, learning_rate=0.01 \n",
      "[CV]  subsample=0.5, n_estimators=50, min_child_weight=7, max_depth=7, learning_rate=0.01, total=   0.2s\n",
      "[CV] subsample=0.5, n_estimators=75, min_child_weight=10, max_depth=6, learning_rate=1.0 \n",
      "[CV]  subsample=0.5, n_estimators=75, min_child_weight=10, max_depth=6, learning_rate=1.0, total=   0.3s\n",
      "[CV] subsample=0.5, n_estimators=75, min_child_weight=10, max_depth=6, learning_rate=1.0 \n",
      "[CV]  subsample=0.5, n_estimators=75, min_child_weight=10, max_depth=6, learning_rate=1.0, total=   0.3s\n",
      "[CV] subsample=0.5, n_estimators=75, min_child_weight=10, max_depth=6, learning_rate=1.0 \n",
      "[CV]  subsample=0.5, n_estimators=75, min_child_weight=10, max_depth=6, learning_rate=1.0, total=   0.3s\n",
      "[CV] subsample=0.5, n_estimators=50, min_child_weight=5, max_depth=7, learning_rate=0.5 \n",
      "[CV]  subsample=0.5, n_estimators=50, min_child_weight=5, max_depth=7, learning_rate=0.5, total=   0.2s\n",
      "[CV] subsample=0.5, n_estimators=50, min_child_weight=5, max_depth=7, learning_rate=0.5 \n",
      "[CV]  subsample=0.5, n_estimators=50, min_child_weight=5, max_depth=7, learning_rate=0.5, total=   0.2s\n",
      "[CV] subsample=0.5, n_estimators=50, min_child_weight=5, max_depth=7, learning_rate=0.5 \n",
      "[CV]  subsample=0.5, n_estimators=50, min_child_weight=5, max_depth=7, learning_rate=0.5, total=   0.2s\n",
      "[CV] subsample=0.2, n_estimators=100, min_child_weight=9, max_depth=10, learning_rate=0.5 \n",
      "[CV]  subsample=0.2, n_estimators=100, min_child_weight=9, max_depth=10, learning_rate=0.5, total=   0.2s\n",
      "[CV] subsample=0.2, n_estimators=100, min_child_weight=9, max_depth=10, learning_rate=0.5 \n",
      "[CV]  subsample=0.2, n_estimators=100, min_child_weight=9, max_depth=10, learning_rate=0.5, total=   0.2s\n",
      "[CV] subsample=0.2, n_estimators=100, min_child_weight=9, max_depth=10, learning_rate=0.5 \n",
      "[CV]  subsample=0.2, n_estimators=100, min_child_weight=9, max_depth=10, learning_rate=0.5, total=   0.2s\n",
      "[CV] subsample=0.9, n_estimators=50, min_child_weight=10, max_depth=3, learning_rate=1.0 \n",
      "[CV]  subsample=0.9, n_estimators=50, min_child_weight=10, max_depth=3, learning_rate=1.0, total=   0.1s\n",
      "[CV] subsample=0.9, n_estimators=50, min_child_weight=10, max_depth=3, learning_rate=1.0 \n",
      "[CV]  subsample=0.9, n_estimators=50, min_child_weight=10, max_depth=3, learning_rate=1.0, total=   0.1s\n",
      "[CV] subsample=0.9, n_estimators=50, min_child_weight=10, max_depth=3, learning_rate=1.0 \n",
      "[CV]  subsample=0.9, n_estimators=50, min_child_weight=10, max_depth=3, learning_rate=1.0, total=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:    6.6s finished\n",
      "   Best params => {'subsample': 0.8, 'n_estimators': 10, 'min_child_weight': 8, 'max_depth': 10, 'learning_rate': 0.5}\n",
      "   Best Score => 0.642\n",
      "                                           Estimator     Score  Degree\n",
      "0  LassoLarsCV(copy_X=True, cv=None, eps=2.220446...  0.684159       2\n",
      "1  XGBRegressor(base_score=0.5, colsample_bylevel...  0.664223       1\n",
      "2  XGBRegressor(base_score=0.5, colsample_bylevel...  0.641606       2\n",
      "3  LassoLarsCV(copy_X=True, cv=None, eps=2.220446...  0.214688       1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Stacking: is a model ensembling technique used to combine information\n",
      "    from multiple predictive models to generate a new model.\n",
      "\n",
      "task:   [regression]\n",
      "metric: [r2_score]\n",
      "\n",
      "model 0: [LassoLarsCV]\n",
      "    ----\n",
      "    MEAN:   [0.67083284]\n",
      "\n",
      "model 1: [XGBRegressor]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stacking 2 models: 100%|██████████| 3/3 [00:00<00:00, 70.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ----\n",
      "    MEAN:   [0.61221785]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from mozinor.baboulinet import Baboulinet\n",
    "\n",
    "cls = Baboulinet(filepath=\"train.csv\", y_col=\"attractiveness\", regression=True)\n",
    "res = cls.babouline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Estimator    LassoLarsCV(copy_X=True, cv=None, eps=2.220446...\n",
       "Score                                                 0.684159\n",
       "Degree                                                       2\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fit1stLevelEstimator    [LassoLarsCV(copy_X=True, cv=None, eps=2.22044...\n",
       "Fit2ndLevelEstimator    XGBRegressor(base_score=0.5, colsample_bylevel...\n",
       "Score                                                            0.603205\n",
       "Degree                                                                  2\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.best_stack_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.59080386115189398: {'Fit1stLevelEstimator': [LassoLarsCV(copy_X=True, cv=None, eps=2.2204460492503131e-16,\n",
       "         fit_intercept=True, max_iter=500, max_n_alphas=1000, n_jobs=1,\n",
       "         normalize=False, positive=False, precompute='auto', verbose=False),\n",
       "   XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
       "          learning_rate=0.5, max_delta_step=0, max_depth=10,\n",
       "          min_child_weight=8, missing=None, n_estimators=10, nthread=-1,\n",
       "          objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
       "          scale_pos_weight=1, seed=0, silent=True, subsample=0.8)],\n",
       "  'Fit2ndLevelEstimator': XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
       "         learning_rate=0.5, max_delta_step=0, max_depth=10,\n",
       "         min_child_weight=8, missing=None, n_estimators=10, nthread=-1,\n",
       "         objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
       "         scale_pos_weight=1, seed=0, silent=True, subsample=0.8)},\n",
       " 0.59813031681779649: {'Fit1stLevelEstimator': [XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
       "          learning_rate=0.5, max_delta_step=0, max_depth=10,\n",
       "          min_child_weight=8, missing=None, n_estimators=10, nthread=-1,\n",
       "          objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
       "          scale_pos_weight=1, seed=0, silent=True, subsample=0.8)],\n",
       "  'Fit2ndLevelEstimator': XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
       "         learning_rate=0.5, max_delta_step=0, max_depth=10,\n",
       "         min_child_weight=8, missing=None, n_estimators=10, nthread=-1,\n",
       "         objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
       "         scale_pos_weight=1, seed=0, silent=True, subsample=0.8)},\n",
       " 0.60320549135570456: {'Fit1stLevelEstimator': [LassoLarsCV(copy_X=True, cv=None, eps=2.2204460492503131e-16,\n",
       "         fit_intercept=True, max_iter=500, max_n_alphas=1000, n_jobs=1,\n",
       "         normalize=False, positive=False, precompute='auto', verbose=False)],\n",
       "  'Fit2ndLevelEstimator': XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
       "         learning_rate=0.5, max_delta_step=0, max_depth=10,\n",
       "         min_child_weight=8, missing=None, n_estimators=10, nthread=-1,\n",
       "         objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
       "         scale_pos_weight=1, seed=0, silent=True, subsample=0.8)}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.dict_stack_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check script file train_solo_model_script.py\n",
      "Check script file train_stack_model_script.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'train_stack_model_script.py'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls.bestModelScript()\n",
    "cls.bestStackModelScript()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on July 2017\n",
    "\n",
    "@author: JulienWuthrich\n",
    "\"\"\"\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model.ridge import RidgeCV\n",
    "\n",
    "from vecstack import stacking\n",
    "\n",
    "# Read the csv file\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "regression = True\n",
    "\n",
    "def rmsle(y_true, y_pred):\n",
    "    assert len(y_true) == len(y_pred)\n",
    "\n",
    "    return np.square(np.log(y_pred + 1) - np.log(y_true + 1)).mean() ** 0.5\n",
    "\n",
    "\n",
    "# Split dependants and independant variables\n",
    "y = data[[\"attractiveness\"]]\n",
    "X = data.drop(\"attractiveness\", axis=1)\n",
    "\n",
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "# Apply Some Featuring\n",
    "poly_reg = PolynomialFeatures(degree=2)\n",
    "\n",
    "# Transform into numpy object\n",
    "x_train = poly_reg.fit_transform(X_train)\n",
    "X_test = poly_reg.fit_transform(X_test)\n",
    "y_test = np.array(y_test.ix[:,0])\n",
    "y_train = np.array(y_train.ix[:,0])\n",
    "\n",
    "# Build model with good params\n",
    "model = LassoLarsCV(copy_X=True, cv=None, eps=2.22)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Scoring\n",
    "if regression:\n",
    "    print('Score on test set:', mean_absolute_error(y_test, y_pred))\n",
    "    print('Score on test set:', math.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "else:\n",
    "    print('Score on test set:', accuracy_score(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on July 2017\n",
    "\n",
    "@author: JulienWuthrich\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import ElasticNetCV, LassoLarsCV, RidgeCV\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "\n",
    "from vecstack import stacking\n",
    "\n",
    "# Read the csv file\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "regression = True\n",
    "if regression:\n",
    "    metric = r2_score\n",
    "else:\n",
    "    metric = accuracy_score\n",
    "\n",
    "# Split dependants and independant variables\n",
    "y = data[[\"attractiveness\"]]\n",
    "X = data.drop(\"attractiveness\", axis=1)\n",
    "\n",
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "# Apply Some Featuring\n",
    "poly_reg = PolynomialFeatures(degree=2)\n",
    "\n",
    "# Transform into numpy object\n",
    "x_train = poly_reg.fit_transform(X_train)\n",
    "x_test = poly_reg.fit_transform(X_test)\n",
    "y_test = np.array(y_test.ix[:,0])\n",
    "y_train = np.array(y_train.ix[:,0])\n",
    "\n",
    "# define lmodels\n",
    "lmodels = [ExtraTreesRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
    "          max_features=0.9, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "          min_impurity_split=None, min_samples_leaf=3, min_samples_split=6,\n",
    "          min_weight_fraction_leaf=0.0, n_estimators=75, n_jobs=1,\n",
    "          oob_score=False, random_state=None, verbose=0, warm_start=False), GradientBoostingRegressor(alpha=0.8, criterion='friedman_mse', init=None,\n",
    "             learning_rate=0.1, loss='lad', max_depth=6, max_features=0.6,\n",
    "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "             min_impurity_split=None, min_samples_leaf=6,\n",
    "             min_samples_split=3, min_weight_fraction_leaf=0.0,\n",
    "             n_estimators=75, presort='auto', random_state=None,\n",
    "             subsample=1.0, verbose=0, warm_start=False), ElasticNetCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True,\n",
    "       l1_ratio=0.7, max_iter=1000, n_alphas=100, n_jobs=1,\n",
    "       normalize=False, positive=False, precompute='auto',\n",
    "       random_state=None, selection='cyclic', tol=0.5, verbose=0)]\n",
    "\n",
    "# build the stack level 1\n",
    "S_train, S_test = stacking(\n",
    "    lmodels, x_train, y_train, x_test,\n",
    "    regression=regression, metric=metric,\n",
    "    n_folds=3, shuffle=True, random_state=0, verbose=1\n",
    ")\n",
    "\n",
    "# build model lvel 2\n",
    "model = ElasticNetCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True,\n",
    "       l1_ratio=0.7, max_iter=1000, n_alphas=100, n_jobs=1,\n",
    "       normalize=False, positive=False, precompute='auto',\n",
    "       random_state=None, selection='cyclic', tol=0.5, verbose=0)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(S_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(S_test)\n",
    "\n",
    "# Scoring\n",
    "if regression:\n",
    "    print('Score on test set:', mean_absolute_error(y_test, y_pred))\n",
    "else:\n",
    "    print('Score on test set:', accuracy_score(y_test, y_pred))\n",
    "# print(metric(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
